# ğŸ¬ SentimentAI â€” Movie Review Analysis Dashboard

> **End-to-end NLP pipeline exploring sentiment classification from Bag-of-Words to BERT** âœ¨

A full-stack ML dashboard for IMDB movie sentiment analysis, featuring 4 models (Naive Bayes, Logistic Regression, Bidirectional LSTM, DistilBERT), live predictions with LIME explanations, and rich data visualizations â€” all powered by a FastAPI backend and a beautiful React frontend.

---

## âœ¨ Features

| Feature | Description |
|---|---|
| ğŸ§  **4 ML Models** | Naive Bayes, Logistic Regression, Bidirectional LSTM, DistilBERT |
| âš¡ **Live Predictor** | Real-time sentiment analysis with confidence gauge |
| ğŸ” **LIME Explanations** | Word-level importance highlighting for every prediction |
| ğŸ“Š **Model Comparison** | Metrics table, ROC curves, radar chart, confusion matrices |
| ğŸ“ˆ **Data Explorer** | Length histograms, sentiment donut, word frequency charts |
| ğŸ› **Error Analysis** | Filterable misclassification table with sarcasm examples |
| ğŸ“ **Demo Mode** | Keyword heuristics â€” works without any trained models |
| ğŸŒ™ **Beautiful UI** | Warm design system with Framer Motion animations |

---

## ğŸ—ºï¸ Architecture Overview

```mermaid
flowchart TD
    A([ğŸ‘¤ User â€” Browser\nhttp://localhost:5173]) --> B[âš›ï¸ React Frontend\nVite + Tailwind + Recharts]

    B --> C{API Request}
    C --> D[ğŸš€ FastAPI Backend\nlocalhost:8000]

    D --> E[ğŸ“¦ Model Loader\nStartup: loads all 4 models]

    E --> F1[ğŸŸ£ Naive Bayes\nnb_pipeline.pkl]
    E --> F2[ğŸ”µ Logistic Regression\nlr_pipeline.pkl]
    E --> F3[ğŸŸ¢ Bidirectional LSTM\nrnn_lstm.h5]
    E --> F4[ğŸŸ¡ DistilBERT\ndistilbert/ dir]

    F1 & F2 & F3 & F4 --> G[ğŸ”¢ Preprocessor\nHTML strip â†’ Lemmatize â†’ Tokens]

    G --> H{Model Type}
    H -- sklearn --> I[TF-IDF Vectorize\nâ†’ predict_proba]
    H -- lstm --> J[Keras Tokenizer\nâ†’ pad_sequences â†’ predict]
    H -- bert --> K[HuggingFace Tokenizer\nâ†’ softmax logits]

    I & J & K --> L[ğŸ” LIME Explainer\nWord importance weights]
    L --> M([ğŸ“¤ PredictResponse\nsentiment + confidence\n+ lime_words + ms])

    M --> B

    style A fill:#a78bfa,color:#fff,stroke:#7c3aed
    style B fill:#5b8fb9,color:#fff,stroke:#3a6d96
    style D fill:#81b29a,color:#fff,stroke:#4a8c6f
    style E fill:#c4b5fd,color:#3d3557,stroke:#7c3aed
    style M fill:#4ade80,color:#fff,stroke:#16a34a
    style G fill:#fde68a,color:#7c5200,stroke:#d97706
    style L fill:#fca5a5,color:#7f1d1d,stroke:#ef4444
```

---

## ğŸ” ML Pipeline Flow

```mermaid
flowchart LR
    A([ğŸ“ Raw Review\nIMDB Text]) --> B[ğŸ§¹ Preprocess\nHTML â†’ lowercase\nstopwords â†’ lemmatize]
    B --> C{Embedding Strategy}
    C --> D1[ğŸ“Š TF-IDF\nfor NB and LR]
    C --> D2[ğŸ”¢ Sequences\nfor LSTM]
    C --> D3[ğŸ¤— BERT Tokens\nfor DistilBERT]
    D1 --> E[ğŸ¯ Predict]
    D2 --> E
    D3 --> E
    E --> F[ğŸ” LIME\nExplanation]
    F --> G([âœ… Result\nsentiment + confidence\n+ word weights])

    style A fill:#a78bfa,color:#fff,stroke:#7c3aed
    style G fill:#4ade80,color:#fff,stroke:#16a34a
    style F fill:#fca5a5,color:#7f1d1d,stroke:#ef4444
    style B fill:#fde68a,color:#7c5200,stroke:#d97706
```

---

## ğŸš€ Quick Start

### Prerequisites

- **Python** 3.9+
- **Node.js** v18+

---

### 1. Clone the repository

```bash
git clone https://github.com/your-username/sentiment-dashboard.git
cd sentiment-dashboard
```

---

### 2. Backend Setup

```bash
cd backend
python -m venv venv

# Windows
venv\Scripts\activate

# macOS / Linux
source venv/bin/activate

pip install -r requirements.txt
```

Download required NLTK data:

```bash
python -c "import nltk; nltk.download('stopwords'); nltk.download('wordnet'); nltk.download('omw-1.4')"
```

Start the API:

```bash
uvicorn main:app --reload --port 8000
```

Backend starts at â†’ **http://localhost:8000**
Swagger docs at â†’ **http://localhost:8000/docs**

> **Note:** Without saved model files in `backend/models/saved/`, the backend runs in **demo mode** using keyword heuristics. This still lets you test the full UI with realistic-looking results.

---

### 3. Frontend Setup

Open a new terminal:

```bash
cd frontend
npm install
npm run dev
```

Frontend starts at â†’ **http://localhost:5173**

---

## ğŸ”— Connecting Real Trained Models

After running your `sentiment_analysis.py` training script, save your models using the helper:

```python
# Add to the end of sentiment_analysis.py
import sys
sys.path.insert(0, '.')
from save_models import save_sklearn, save_lstm, save_distilbert, save_keras_tokenizer

save_sklearn(nb_pipeline, 'naive_bayes_pipeline')
save_sklearn(best_lr_model, 'logistic_regression_pipeline')
save_keras_tokenizer(tokenizer)
save_lstm(rnn_model)
save_distilbert(model_bert, tokenizer_bert)
```

For TensorFlow / Transformers support, uncomment in `backend/requirements.txt`:

```
tensorflow==2.15.0
transformers==4.36.2
```

### Model File Locations

| Model | File(s) | Path |
|---|---|---|
| Naive Bayes | `naive_bayes_pipeline.pkl` | `backend/models/saved/` |
| Logistic Regression | `logistic_regression_pipeline.pkl` | `backend/models/saved/` |
| LSTM | `rnn_lstm.h5` + `tokenizer.pkl` | `backend/models/saved/` |
| DistilBERT | entire directory | `backend/models/saved/distilbert/` |

---

## ğŸ“ Project Structure

```
sentiment-dashboard/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py                  # FastAPI app + all API routes
â”‚   â”œâ”€â”€ schemas.py               # Pydantic request/response models
â”‚   â”œâ”€â”€ requirements.txt         # Python dependencies
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ loader.py            # Loads all 4 models on startup
â”‚   â”‚   â””â”€â”€ saved/               # Place .pkl / .h5 / distilbert/ files here
â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ preprocess.py        # Text cleaning pipeline (HTML â†’ lemmatize)
â”‚       â”œâ”€â”€ predict.py           # Inference logic for all model types
â”‚       â””â”€â”€ explain.py           # LIME explanation generation
â”‚
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ pages/
â”‚       â”‚   â”œâ”€â”€ Home.jsx             # Hero, stat cards, pipeline diagram
â”‚       â”‚   â”œâ”€â”€ DataExplorer.jsx     # Charts + paginated review table
â”‚       â”‚   â”œâ”€â”€ LivePredictor.jsx    # Textarea â†’ model â†’ confidence gauge
â”‚       â”‚   â”œâ”€â”€ ModelComparison.jsx  # Metrics, ROC, radar, confusion matrix
â”‚       â”‚   â””â”€â”€ ErrorAnalysis.jsx    # Filterable misclassification table
â”‚       â”œâ”€â”€ components/
â”‚       â”‚   â”œâ”€â”€ Navbar.jsx
â”‚       â”‚   â”œâ”€â”€ StatCard.jsx
â”‚       â”‚   â”œâ”€â”€ ConfidenceGauge.jsx  # SVG arc gauge
â”‚       â”‚   â”œâ”€â”€ LimeHighlighter.jsx  # Color-coded word importance
â”‚       â”‚   â”œâ”€â”€ ModelSelector.jsx    # Multi-model toggle
â”‚       â”‚   â””â”€â”€ charts/
â”‚       â”‚       â”œâ”€â”€ MetricsBar.jsx
â”‚       â”‚       â”œâ”€â”€ ROCCurve.jsx
â”‚       â”‚       â”œâ”€â”€ RadarChart.jsx
â”‚       â”‚       â””â”€â”€ ConfusionMatrix.jsx
â”‚       â”œâ”€â”€ hooks/
â”‚       â”‚   â””â”€â”€ usePrediction.js     # Custom hook for API calls + state
â”‚       â””â”€â”€ services/
â”‚           â””â”€â”€ api.js               # Axios client (VITE_API_URL)
â”‚
â”œâ”€â”€ save_models.py               # Helper to export trained models for the API
â””â”€â”€ sentiment_analysis.py        # Original training script
```

---

## ğŸ“¡ API Reference

### Prediction

| Method | Endpoint | Description |
|---|---|---|
| `POST` | `/predict` | Single model prediction + LIME explanation |
| `POST` | `/predict/compare` | Run all 4 models on the same input |

**Example request:**

```bash
curl -X POST http://localhost:8000/predict \
  -H "Content-Type: application/json" \
  -d '{"text": "This movie was absolutely fantastic!", "model": "logistic_regression"}'
```

**Example response:**

```json
{
  "model": "logistic_regression",
  "sentiment": "positive",
  "confidence": 0.9312,
  "lime_words": [
    { "word": "fantastic", "weight": 0.8421 },
    { "word": "absolutely", "weight": 0.3109 }
  ],
  "inference_time_ms": 12.4
}
```

### Analytics & Data

| Method | Endpoint | Description |
|---|---|---|
| `GET` | `/metrics` | Pre-computed test set metrics for all 4 models |
| `GET` | `/dataset/stats` | EDA data (lengths, word freqs, sample reviews) |
| `GET` | `/errors` | Misclassifications â€” filterable by `model`, `true_label`, `pred_label` |
| `GET` | `/training-history` | Loss/accuracy per epoch (`?model=rnn_lstm` or `distilbert`) |
| `GET` | `/` | Health check |

---

## ğŸ¨ Pages

| Page | Route | What's Inside |
|---|---|---|
| **Home** | `/` | Hero, stat cards, pipeline diagram, tech stack grid |
| **Data Explorer** | `/data` | Length histograms, sentiment donut, word frequency bars, paginated review table |
| **Live Predictor** | `/predictor` | Textarea input, model selector, confidence gauge, LIME word highlighting |
| **Model Comparison** | `/comparison` | Metrics table, grouped bars, ROC curves, radar chart, confusion matrices, training history |
| **Error Analysis** | `/errors` | Filterable misclassification table, high-confidence failures, error-by-length chart, sarcasm examples |

---

## ğŸ“Š Model Performance

| Model | Accuracy | Precision | Recall | F1-Score | AUC-ROC |
|---|---|---|---|---|---|
| ğŸŸ£ **Naive Bayes** | 87.2% | 87.6% | 86.7% | 87.1% | 0.950 |
| ğŸ”µ **Logistic Regression** | 90.4% | 91.0% | 89.7% | 90.3% | 0.967 |
| ğŸŸ¢ **RNN (LSTM)** | 91.2% | 91.8% | 90.5% | 91.1% | 0.972 |
| ğŸŸ¡ **DistilBERT** â­ | **93.2%** | **93.5%** | **92.8%** | **93.1%** | **0.986** |

---

## ğŸ› ï¸ Tech Stack

**Frontend**

| Technology | Purpose |
|---|---|
| React 18 + Vite | UI framework + fast dev server |
| Tailwind CSS | Utility-first styling with custom warm palette |
| Recharts | Line, bar, pie, radar chart components |
| Framer Motion | Smooth page and element animations |
| React Router v6 | Client-side routing |
| Axios | HTTP client with base URL config |
| react-hot-toast | Non-intrusive notification toasts |
| lucide-react | Clean icon set |

**Backend**

| Technology | Purpose |
|---|---|
| FastAPI | Async Python API framework |
| Pydantic v2 | Request/response schema validation |
| uvicorn | ASGI server |
| scikit-learn | Naive Bayes + Logistic Regression pipelines |
| TensorFlow/Keras | Bidirectional LSTM model |
| HuggingFace Transformers | DistilBERT fine-tuning + inference |
| LIME | Local Interpretable Model-agnostic Explanations |
| NLTK | Stopword removal + lemmatization |

---

## âš™ï¸ Environment Variables

**Frontend** â€” create `frontend/.env`:

```env
VITE_API_URL=http://localhost:8000
```

For production, point this to your deployed backend URL.

---

## ğŸš¢ Deployment

### Backend (e.g. Render)

1. Set all Python environment variables in your host dashboard
2. Install command: `pip install -r requirements.txt`
3. Start command: `uvicorn main:app --host 0.0.0.0 --port $PORT`
4. For TF/BERT models, ensure your instance has enough RAM (â‰¥ 4 GB recommended)

### Frontend (e.g. Vercel)

1. Set `VITE_API_URL` to your deployed backend URL (e.g. `https://your-backend.onrender.com`)
2. Build command: `npm run build`
3. Output directory: `dist`
4. Vercel auto-handles client-side routing rewrites

---

## ğŸ› Known Issues / Common Gotchas

- âœ… Demo mode runs without any model files â€” great for UI testing
- âœ… LIME falls back to keyword heuristics for non-sklearn models (LSTM, DistilBERT)
- âš ï¸ TensorFlow and Transformers are commented out in `requirements.txt` by default â€” uncomment to enable LSTM + DistilBERT
- âš ï¸ DistilBERT requires ~1.5 GB RAM to load â€” demo mode is recommended for lightweight hosting
- âš ï¸ LIME explanations add ~200â€“500ms to inference time for sklearn models

---

## ğŸ“„ License

MIT â€” free to use, modify, and distribute.
